


import pandas as pd
from pathlib import Path
pd.__version__


filepath = Path('../../datasets/Ch2/movieboxoffice.csv')


ts = pd.read_csv(filepath,
                 header=0,
             
                 parse_dates=[0],
                 index_col=0,
                 usecols=['Date',
                          'DOW',
                          'Daily',
                          'Forecast',
                          'Percent Diff'])
ts.head()


ts.info()





clean = lambda x: x.str.replace('[^\d]', '', regex=True)                                                      
c_df = ts[['Daily', 'Forecast']].apply(clean, axis=1)
ts[['Daily', 'Forecast']] = c_df.astype(float)
                                


ts.head()


ts.info()


ts.memory_usage()


ts.memory_usage().sum()





ts.dtypes


date_parser = lambda x: pd.to_datetime(x, format="%d-%b-%y")
ts = pd.read_csv(filepath,
                 parse_dates=[0],
                 index_col=0,
                 date_parser=date_parser,
                 usecols=[0,1,3, 7, 6])
ts.head()


ts = pd.read_csv(filepath,
                 header=0,
                 parse_dates=['Date'],
                 index_col=0,
                 infer_datetime_format= True,
                 usecols=['Date',
                          'DOW',
                          'Daily',
                          'Forecast',
                          'Percent Diff'])
ts.head()








#!conda install openpyxl -y
#!pip install openpyxl


import pandas as pd
from pathlib import Path
filepath = Path('../../datasets/Ch2/sales_trx_data.xlsx')


import openpyxl
openpyxl.__version__


excelfile = pd.ExcelFile(filepath)
excelfile.sheet_names


excelfile.parse('2017')


ts = pd.read_excel(filepath,
                    engine='openpyxl',
                    index_col=1,
                    sheet_name=[0,1],
                    parse_dates=True)
ts.keys()


ts = pd.read_excel(filepath,
                    engine='openpyxl',
                    index_col=1,
                    sheet_name=['2017','2018'],
                    parse_dates=True)
ts.keys()


ts = pd.read_excel(filepath,
                    engine='openpyxl',
                    index_col=1,
                    sheet_name=None,
                    parse_dates=True)
ts.keys()


ts['2017'].info()


ts['2018'].info()


ts_combined = pd.concat([ts['2017'],ts['2018']])


ts_combined.info()


pd.concat(ts).index


ts_combined = pd.concat(ts).droplevel(level=0)
ts_combined.head()


ts = pd.read_excel(filepath,
                   index_col=1,
                   sheet_name='2018',
                   parse_dates=True)
type(ts)





excelfile = pd.ExcelFile(filepath)
excelfile.parse(sheet_name='2017',
                index_col=1,
                parse_dates=True).head()






#!conda install boto3 s3fs html5lib lxml -y
#!pip install boto3 s3fs html5lib lxml





import pandas as pd


import pandas as pd
import boto3, s3fs, lxml
print(f'''
pandas -> {pd.__version__}
boto3 -> {boto3.__version__}
s3fs -> {s3fs.__version__}
lxml -> {lxml.__version__}
''')





# example of produced error
url = 'https://github.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook./blob/main/datasets/Ch2/AirQualityUCI.csv'
# pd.read_csv(url)



url = 'https://raw.githubusercontent.com/PacktPublishing/Time-Series-Analysis-with-Python-Cookbook/main/datasets/Ch2/AirQualityUCI.csv'
date_parser = lambda x: pd.to_datetime(x, format="%d/%m/%Y")

df = pd.read_csv(url,
                 delimiter=';',
                 index_col='Date',
                 date_parser=date_parser)

df.iloc[:3,1:4]








url = 'https://s3.us-east-1.amazonaws.com/tscookbook/AirQualityUCI.xlsx'

df = pd.read_excel(url,
                   index_col='Date',
                   parse_dates=True)
df.head()





url = 'https://tscookbook.s3.amazonaws.com/AirQualityUCI.xlsx'
df = pd.read_excel(url,
                   index_col='Date',
                   parse_dates=True)
df.head()





s3uri = 's3://tscookbook/AirQualityUCI.xlsx'
df = pd.read_excel(s3uri,
                   index_col='Date',
                   parse_dates=True)
df.head()





import configparser
config = configparser.ConfigParser()
config.read('aws.cfg')

AWS_ACCESS_KEY = config['AWS']['aws_access_key']
AWS_SECRET_KEY = config['AWS']['aws_secret_key']



s3uri = "s3://tscookbook-private/AirQuality.csv"

df = pd.read_csv(s3uri,
                 index_col='Date',
                 parse_dates=True,
                 storage_options= {
                         'key': AWS_ACCESS_KEY,
                         'secret': AWS_SECRET_KEY
                     })


df.iloc[:3, 1:4]





import boto3
bucket = "tscookbook-private"
client = boto3.client("s3",
                  aws_access_key_id =AWS_ACCESS_KEY,
                  aws_secret_access_key = AWS_SECRET_KEY)



data = client.get_object(Bucket=bucket, Key='AirQuality.csv')


data.keys()


df = pd.read_csv(data['Body'],
                 index_col='Date',
                 parse_dates=True)



df.iloc[:3, 1:4]





url = "https://en.wikipedia.org/wiki/COVID-19_pandemic_by_country_and_territory"
results = pd.read_html(url)
print(len(results))



# for i, k in enumerate(results):
#     print(i)
#     display(k.head())
    


df = results[15]
df.columns


df[['Total cases', 'Total deaths', 'Cases per million']].head()





import pandas as pd
html = """
 <table>
   <tr>
     <th>Ticker</th>
     <th>Price</th>
   </tr>
   <tr>
     <td>MSFT</td>
     <td>230</td>
   </tr>
   <tr>
     <td>APPL</td>
     <td>300</td>
   </tr>
     <tr>
     <td>MSTR</td>
     <td>120</td>
   </tr>
 </table>

 </body>
 </html>
 """

df = pd.read_html(html)
df[0]






#!conda install html5lib beautifulSoup4


import pandas as pd
url = "https://en.wikipedia.org/wiki/COVID-19_pandemic_by_country_and_territory"
df = pd.read_html(url, attrs={'class': 'sortable'})
len(df)


df[3].columns





import pandas as pd
path = '../../datasets/Ch2/DCSKINPRODUCT.sas7bdat'



df = pd.read_sas(path, chunksize=10000)
type(df)



results = []
for chunk in df:
    results.append(
        chunk)
len(results)


df = pd.concat(results)
df.shape


df = pd.read_sas(path, chunksize=10000)
results = []
for chunk in df:
    results.append(
        chunk.groupby('DATE')['Revenue']
             .agg(['sum', 'count']))
len(results)


pd.concat(results).shape


results[0].loc['2013-02-10']


results[1].loc['2013-02-10']


results[2].loc['2013-02-10']


from functools import reduce
final = reduce(lambda x1, x2: x1.add(x2, fill_value=0), results)
type(final)


final.loc['2013-02-10']


final.shape





# !conda install dask # install everything
#!conda install dask-core # install only core parts of dash
#!python -m pip install "dask[complete]"    # Install everything
#!python -m pip install dask                # Install only core parts of dask








import pandas as pd
from pathlib import Path
pd.__version__





# file = 'https://www.ncei.noaa.gov/orders/cdo/3352259.csv'
# df = pd.read_csv(file)
# df['DT'] = pd.to_datetime(df['DATE'])
# df['year'] = df['DT'].dt.year
# df.to_parquet('../../datasets/Ch2/LA_weather.parquet', engine='pyarrow', partition_cols=['year'], compression='snappy')





file = Path('../../datasets/Ch2/LA_weather.parquet/')
df = pd.read_parquet(file,
                    engine='pyarrow')


df.info()





filters = [('year', '==', 2012)]
df_2012 = pd.read_parquet(file, 
                          engine='pyarrow', 
                          filters=filters)





df_2012.info()


filters = [('year', '>', 2020)]

df = pd.read_parquet(file, 
                     engine='pyarrow', 
                     filters=filters)
df.info()


filters = [('year', '>=', 2021)]
pd.read_parquet(file, 
                engine='pyarrow', 
                filters= filters).info()


filters = [('year', 'in', [2021, 2022, 2023])]
df = pd.read_parquet(file, 
                     engine='pyarrow', 
                     filters=filters)
df.info()


df.head()


columns = ['DATE', 'year', 'TMAX']
df = pd.read_parquet(file, 
                     engine='pyarrow', 
                     filters=filters, 
                     columns=columns)

df.head()


df.info()





pa.parquet.read_table()


import pyarrow.parquet as pq
import pyarrow as pa
from pathlib import Path

file = Path('../../datasets/Ch2/LA_weather.parquet/')
table = pq.read_table(file, filters=filters, columns=columns)


import pyarrow as pa
isinstance(table, pa.Table)


df = table.to_pandas()
df.info()


df.head()


table.column_names


table.schema


table.column_names


type(table)


table.schema.pandas_metadata


pq_dataset = pq.ParquetDataset(file, filters=filters)


pq_dataset


type(pq_dataset)


pq_dataset.files


pq_dataset.schema


pq_dataset.schema.pandas_metadata


pq_dataset.schema.metadata


data = pq_dataset.read()


type(data)


data


data.num_columns


data.num_rows


data.schema


file = Path('../../datasets/Ch2/LA_weather.parquet/')
table = pq.read_table(file)


table.num_rows


table.to_pandas()


pa.Table.from_pandas(df)





columns = ['DATE','year', 'TMAX']
filters = [('year', 'in', [2021, 2022, 2023])]

tb = pq.read_table(file,
                   filters=filters, 
                   columns=columns,
                   use_pandas_metadata=True)

df_pa = tb.to_pandas()


df_pa.info()


df_pd = pd.read_parquet(file, 
                        filters=filters, 
                        columns=columns,
                        use_pandas_metadata=True)

df_pd.info()


schema = pa.schema([
    ('DATE', pa.string()),
    ('year', pa.int64()),
    ('TMAX', pa.int64())
])







